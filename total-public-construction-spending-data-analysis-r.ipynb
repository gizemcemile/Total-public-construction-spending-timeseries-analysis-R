{"cells":[{"metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"cell_type":"code","source":"# This R environment comes with many helpful analytics packages installed\n# It is defined by the kaggle/rstats Docker image: https://github.com/kaggle/docker-rstats\n# For example, here's a helpful package to load\n\nlibrary(tidyverse) # metapackage of all tidyverse packages\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nlist.files(path = \"../input\")\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we were a statistics student at METU. We chose time series analysis as an elective course. Ceylan Yozgatlıgil was our professor. She gived us a guideline. We copied the step, results and the comments from our R-markdown. This is not a perfect project, but it may help you all :D [http://users.metu.edu.tr/ceylan/STAT%20497.htm]"},{"metadata":{},"cell_type":"markdown","source":"# STEPS TIME SERİES DATA ANALYSİS\nMake sure that you have at least 50 data points in your dataset. Keep last 4 (for yearly and quarterly) or 12 (for monthly) observations as test data. Your data should be up to date. \n1.\tIntroduction covering data descripition, aim and the source of data.\n2.\tTime series plot and interpretation (Visually determine the existence of a trend, seasonality, outliers).\n3.\tKeep several observations out of the analysis to use them to measure the forecast accuracy of the models. (For yearly data and quarterly data 4 or 5, monthly data 12).\n4.\tBox-Cox transformation analysis: If the series need any transformation, do it. If the information criterion values are too close to each other, don’t transform the data.\n5.\tMake a anomaly detection and if necessary clean the series from anomalies (use anomalize, forecast (tsclean function) or AnomalyDetection packages).\n6.\tACF, PACF plots, KPSS and ADF or PP test results  for zero mean, mean and trend cases and their interpretation. For seasonal unit root, HEGY and OCSB or Canova-Hansen tests are required.\n7.\tIf there is a trend, remove it either by detrending or differencing. You may need to apply unit root tests again.\n8.\tThen, look at the time series plot of a stationary series, ACF and PACF plots, information table, ESACF.\n9.\tIdentify a proper ARMA or ARIMA model or SARIMA model.\n10.\tAfter deciding the order of the possible model (s), run MLE or conditional or uncondinitional LSE and estimate the parameters. Compare the information criteria of several models. (Note: If there is a convergence problem, you can change your estimation method).\n11.\tDiagnostic Checking: \na)\tOn the residuals, perform portmanteau lack of fit test, look at the ACF-PACF plots of the resuduals (for all time points, ACF and PACF values should be in the white noise bands), look at the standardized residuals vs time plot to see any outliers or pattern. \nb)\tUse histogram, QQ-plot and Shapiro-Wilk test (in ts analysis, economists prefer Jarque-Bera test) to check normality of residuals. \nc)\tPerform Breusch-Godfrey test for possible autocorrelation in residual series. The result should be insignificant.\nd)\tFor the Heteroscedasticity, look at the ACF-PACF plots of the squared residuals (there should be no significant spikes); perform ARCH Engle's Test for Residual Heteroscedasticity under aTSA package. The result should be insignificant. If the result is significant, you can state that the error variance is not constant and it should be modelled, but don’t intend to model the variance. If there is a heteroscedasticity problem, most probably normality test on residuals will fail. The high values in the lower and upper extremes destroy the normality due to high variation. In your project, you can state these only. When solving a real life problem, you cannot just state and quit dealing this problem!\n12.\tForecasting: The number of forecasts should be same as the length of your test data.\na.\tPerform Minimum MSE Forecast for the stochastic models (like ARIMA or SARIMA) \nb.\tUse ets code under the forecast package to choose the best exponential smoothing (simple, Holt’s, Holt-Winter’s) method that suits your series for deterministic forecasting. \nc.\tObtain forecasts using neural networks (nnetar)\n13.\tIf you transformed the series for SARIMA model, back transform the series to reach the estimates for the original units. \n14.\tCalculate the forecast accuracy measures and state which model gives the highest performance for your dataset.\n15.\tProvide plots of the original time series, predictions, forecasts and prediction intervals on the same plot drawing the forecast origin for ARIMA models, exponential smoothing method and neural networks. The plot for each model should look like the following plot.\n16.CONCLUSION"},{"metadata":{"trusted":true},"cell_type":"code","source":"PBNRESCONS <- ts(read.csv(\"../input/pbnrescons/PBNRESCONS.csv\"),start = 2002,frequency = 12)\ndata<-ts(PBNRESCONS[,2],start=2002,frequency = 12)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The project dataset is downloaded from fred.stlouisfed.org. This data provides information about monthly estimates of the total dollar value of construction in the U.S from 2002 to 2019. As cited in Census.gov, data estimates include the cost of labor and materials, cost of architectural and engineering work, overhead costs, interest and taxes paid during construction, and contractor’s profits. A million dollars define units of the value of spending. The project aims that develop a time series analysis using total public construction spendings information and offer an efficient model to use for generating future expenses."},{"metadata":{},"cell_type":"markdown","source":"> Before starting any analysis, it is better to look at the plot of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"par(mfrow=c(3,1))\nplot(data)\nacf(as.numeric(data))\npacf(as.numeric(data))\n#plot shows an increasing trend. ACF shows slow decay that is an indication of being non-stationarity. Also, we know that since our data set is monthly, it is seasonal.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at this plot, it can be said that the data has an increasing trend until 2010. However, between 2010-2015 there are factors that cause decline in trend. Later, there is also an increasing trend. It is also better to look at ACF and PACF plots."},{"metadata":{},"cell_type":"markdown","source":"ACF plot shows slow decay that is an indication of being non-stationarity. Non-stationarity means that the mean and the variance of the observations change over time. Also, it is known that the series is seasonal because the data set is monthly.\nFirstly, it is necessary to divide data into two as train and test to measure the forecast accuracy of the models.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train=window(data,start = 2002,end=c(2018,9),frequency = 12)\ndata_test=window(data,start=c(2018,10),end=c(2019,9),frequency=12)\npar(mfrow=c(3,1))\nplot(data_train)\nacf(as.numeric(data_train))\npacf(as.numeric(data_train))\n#plot shows an increasing trend. ACF shows slow decay that is an indication of being non-stationarity. Also, we know that since our data set is monthly, it is seasonal.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Secondly, Box-Cox transformation should be done to stabilize variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"library(forecast)\nlambda <- BoxCox.lambda(data_train)\nlambda # lambda is not equal to one, so we need to use transformed data\ntrain_t=BoxCox(data_train,lambda)\npar(mfrow=c(3,1))#After transforming the data, the plot still shows an increasing trend. \nplot(train_t)\nacf(as.numeric(train_t))#ACF shows slow decay that is an indication of being non-stationarity.\npacf(as.numeric(train_t))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to detect unusual observations, anomaly detection is used. Furthermore, one observation is detected as an anomaly. After that this anomaly observation is cleaned from data."},{"metadata":{"trusted":true},"cell_type":"code","source":"library(chron)\ntime=as.chron(train_t)\ntime\ntime1=as.Date(time,format=\"%d-%b-%y\") #format is important.\ntime1\n#After this we convert our train data which is ts object into data frame.\ntrain_bc_anomaly=data.frame(trainbc=train_t)\nhead(train_bc_anomaly)\n#Then, add time1 object as rownames to data frame created for anomaly detection.\nrownames(train_bc_anomaly)=time1\nhead(train_bc_anomaly)\nlibrary(anomalize) #tidy anomaly detection\nlibrary(tidyverse) #tidyverse packages like dplyr, ggplot, tidyr\ntrain_bc_anomaly_ts <- train_bc_anomaly %>% rownames_to_column() %>% as.tibble() %>% \n  mutate(date = as.Date(rowname)) %>% select(-one_of('rowname'))\nhead(train_bc_anomaly_ts)\n#The dataset is prepared for the anomaly detection.\ntrain_bc_anomaly_ts %>% \n  time_decompose(trainbc, method = \"stl\", frequency = \"auto\", trend = \"auto\") %>%\n  anomalize(remainder, method = \"gesd\", alpha = 0.05, max_anoms = 0.2) %>%\n  plot_anomaly_decomposition()\ntrain_bc_clean=tsclean(train_t)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to detect unusual observations, anomaly detection is used. Furthermore, one observation is detected as an anomaly. After that this anomaly observation is cleaned from data."},{"metadata":{"trusted":true},"cell_type":"code","source":"par(mfrow=c(3,1))\nplot(train_t)\nacf(as.numeric(train_t))\npacf(as.numeric(train_t))\n\n#I should test whether it has unit root or not.\n#KPSS\nlibrary(tseries)\nkpss.test(train_bc_clean,null=c(\"Level\"))\n#H0:stationary.Since p-value is less than 0.05, we reject H0 and we can say that the series is not stationary\nkpss.test(train_bc_clean,null=c(\"Trend\"))\n#H0: There is a deterministic trend. H1: There is a stochastic trend.Since p-value is less than 0.05, we reject H0 and we can say that there is a stochastic trend.\n#ADF\nlibrary(fUnitRoots)\nadfTest(train_bc_clean, lags=1, type=\"c\") #I select lag 1 because of PACF. In PACF only 1st lag is significant. #Because time series plot of the process shows that the mean of the system is not 0 or close to 0. In such cases, we will prefer to use this test with c.\n#Ho: The process has unit root (non-stationary). H1: The process is stationary.Since p value is greater than 0.05, we fail to reject H0. It means that we don’t have enough evidence to claim that we have a stationary system.\nadfTest(train_bc_clean, lags=1, type=\"ct\") #\"ct\" is prefered because intercept and time trend exist.\n#Our process has unit root.According to ADF, our process has unit root. \n#As a result, It means we have a non-stationary series with stochastic trend. To remove the trend, we apply differencing using diff function later.\nlibrary(tseries)\npp.test(train_bc_clean)\n#Ho: The process has unit root (non-stationary). H1: The process is stationary.Since p value is greater than 0.05, we fail to reject H0. It means that we don’t have enough evidence to claim that we have a stationary system.\n#Kpss, ADF and PP Tests show that there is a unit root problem in the system.\n#To solve this,take the difference.\n\n#HEGY Test\nlibrary(uroot)\nhegy.test(train_bc_clean,deterministic = c(1,1,1),maxlag=12)\n#In this output, we will use p value of t_1 for regular unit root and use the p value of F_11:12 for testing seasonal unit root.\n#The output shows that the system does not have regular unit roota nd seasonal unit root because p value of t_1 is less than 0.05 and f_11:12 is less than 0.05\n#Canova-Hensen Test\nch.test(train_bc_clean,type = \"dummy\",sid=c(1:12))\n#according to Canova Hensen test,we do not have a seasonal unit root problem","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After anomaly detection and cleaning data process, the system still looks non-stationary. For this reason, it is better to look at KPSS test.\nH0: The system is stationary. H1: The system is non-stationary.\nIt can said that the system is non-stationary since p-value is less 0.05. Now, it can be looked at trend of the data.\n"},{"metadata":{},"cell_type":"markdown","source":"*KPSS test for trend stationarity\nH0: There is a deterministic trend. H1: There is a stochastic trend.\nSince p-value is less than 0.05, we reject H0, and we can say that there is a stochastic trend.\nADF test and PP test are used to check unit root problems.\nHo: The process has unit root (non-stationary). H1: The process is stationary.\nSince p value is greater than 0.05, we fail to reject H0. It means that we don’t have enough evidence to claim that we have a stationary system.\nHo: The process has unit root (non-stationary). H1: The process is stationary.\nSince p value is greater than 0.05, we fail to reject H0. It means that we don’t have enough evidence to claim that we have a stationary system.\nKPSS, ADF and PP Tests show that there is a unit root problem in the system.\nFor seasonal unit root, HEGY and Canova-Hansen tests are used. The result below is from HEGY test.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"library(forecast)\nndiffs(train_bc_clean)\ndtrain=diff(train_bc_clean) #regular difference\nnsdiffs(diff(train_bc_clean))#no need to seasonal difference\nplot(dtrain,main=\"Time Series Plot of the Differenced Data\",col=\"red\")\n# seems stationary but let's check it with kpss test\nkpss.test(dtrain,null=c(\"Level\"))\n#Since the p-value is greater than 0.05,the process is stationary\nhegy.test(dtrain)\n#there is no regular unit root and no seasonal unit root","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nIn this output, p value of tpi_1 gives the result for regular unit root and the p value of Fpi_11:12 gives the result for testing seasonal unit root.\nThe output shows that the system has both regular and seasonal unit roots because both of the p-values of tpi_1 and Fpi_11:12 are greater than 0.05.\nCanova and Hansen (CH) test statistic for the null hypothesis of a stable seasonal pattern.\nThe output above shows that according to Canova-Hansen test, the seasonal pattern is stable.\nIn order to remove the trend, differencing is necessary. Then, regular differencing is taken. Taking regular difference did not solved the regular unit root. (p-value of tpi_1 is greater than 0.05) Therefore, seasonal differencing is also taken. \nThe process seems stationary around mean 0 after taking differences. But, it is better to check this with KPSS test.\nSince the p-value is greater than 0.05,the process is stationary. \nTaking both regular and seasonal differences solved both regular and seasonal unit root problems (p-values of tpi_1 and Fpi_11:12 are less than 0.05)"},{"metadata":{"trusted":true},"cell_type":"code","source":"par(mfrow=c(2,1))\nacf(as.vector(dtrain),main=\"ACF Of Differenced Data\",col=\"red\",lag.max = 36)\npacf(as.vector(dtrain),main=\"PACF Of  Differenced Data\",col=\"red\",lag.max = 36)\n#By looking at ACF and PACF plots,we can suggest SARIMA(0,1,0)(2,0,2)[12]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model SARIMA(2,1,2)(2,1,2)[12] can be suggested by looking at ACF and PACF graphs of differenced data. In addition, from auto.arima the model SARIMA(1,1,1)(2,0,2)[12] can be suggested. Now, it is time to check significance of these models.To do so, we will look at the last estimated parameters of each component. If the ratio between these estimates and their standard errors (s.e) are greater than +2 or less than -2 ,it can said that these parameters are significant and the model is significant."},{"metadata":{"trusted":true},"cell_type":"code","source":"install.packages('TSA')\nlibrary(TSA)\n#ESACF Table\neacf(dtrain)\n#From ESACF table,we can suggest ARIMA(3,1,3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#SARIMA(0,1,0)(2,0,2)[12]\n#ARIMA(3,1,1)\n#ARIMA(2,1,0)\n#SARIMA(1,1,1)(2,0,2)[12]"},{"metadata":{"trusted":true},"cell_type":"code","source":"install.packages('caschrono')\nlibrary(caschrono)\narmaselect(dtrain)#MINIC Table\n#From MINIC table,we can suggest ARIMA(2,1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"library(forecast)\nauto.arima(train_bc_clean)\n#ARIMA(1,1,1)(2,0,2)[12]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, this model is not significant."},{"metadata":{"trusted":true},"cell_type":"code","source":"fit1<-Arima(train_bc_clean,order = c(0, 1, 0),seasonal=list(order=c(2,0,2),period=12))\nsummary(fit1)\nratio_sar2=-0.1632/0.3393\nratio_sar2\nratio_sma2=-0.0815/0.3395\nratio_sma2\n#the model is insignificant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit2<-Arima(train_bc_clean,order = c(3, 1, 3))\nsummary(fit2)\nratio_ar3=-0.1609/0.3899\nratio_ar3\nratio_ma3=-0.0318/0.4291\nratio_ma3\n#Since both AR3 and MA1 are in the inerval (-2,2),the model is insignificant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit3<-Arima(train_bc_clean,order = c(2, 1, 0))\nsummary(fit3)\nratio_ar2=-0.1066/0.0721\nratio_ar2\n#the model is insignificant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit4<-Arima(train_bc_clean,order = c(1, 1, 1),seasonal=list(order=c(2,0,2),period=12))\nsummary(fit4)\nratio_ar1=-0.0525/0.0046\nratio_ar1\nratio_ma1=0.0276/ 0.0131\nratio_ma1\nratio_sar2=-0.2488/0.0100\nratio_sar2\nratio_sma2=0.0743/0.0079\nratio_sma2\n#When each parameter estimates is divided by their standard errors,it is understood that the approximated test statistics are greater than 2 for all parameters. \n#That's why, it is said that the model is significant","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since only SARIMA(1,1,1)(2,0,2)[12] model is significant, we continue diagnoctic check with this model."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(y=resid(fit4),x=as.vector(time(data_train)), ylab='Residuals',xlab='Time',type='o')\nabline(h=0)\n#It seems that the residuals seems stationary around zero. \npar(mfrow=c(1,2))\nr=resid(fit4)\nacf(r)\npacf(r)\n#standardized residuals vs time\nStandard.res=rstandard(fit4)\nplot(y=Standard.res,x=as.vector(time(data_train)), ylab='Standardized Residuals',xlab='Time',type='o')\n#They are scattered around zero and it can be interpreted as zero mean.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Since only SARIMA(1,1,1)(2,0,2)[12] model is significant, we continue diagnostic check with this model. Let us look at ACF and PACF plots of residuals.\n2. For all time points, ACF and PACF values should be in the white noise bands. However, some of the spikes are outside of the bands. This means there might be some outliers.\n3. The standardized residual vs time plot also shows some outliers."},{"metadata":{},"cell_type":"markdown","source":"#Checking the normality of the residual"},{"metadata":{"trusted":true},"cell_type":"code","source":"#QQ Plot\nqqnorm(r)\nqqline(r)\n#Since the points fall aproximately along the qq-line, we can say that residuals are normally distributed\n\n#Histogram\nhist(r,main=\"Histogram of Residuals\",breaks=20)\n#The histogram of residual shows that they might have a normal distribution with outliers.\n\n#Shapiro-Wilk test & Jarque-Bera test\n#Ho: Residuals have normal distribution.\n#H1: Residuals don't have normal distribution.\nshapiro.test(r)\nlibrary(tseries)\njarque.bera.test(r)\n#p-values are grater than 0.05, so fail to reject H0.\n#We have enough evidence to claim that residuals have normal distribution.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* By looking at Normal Q-Q Plot, it can be said that residuals are normally distributed.\n* The histogram of residuals shows that they might have a normal distribution with outliers.\n* Ho: Residuals have normal distribution.H1: Residuals do not have normal distribution.\n* p-value is greater than 0.05, so fail to reject H0. We don’t have enough evidence to claim that residuals don’t have normal distribution.\n"},{"metadata":{},"cell_type":"markdown","source":"#Checking Autocorrelation of Residuals"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Breusch-Godfrey Test\nlibrary(TSA)\nlibrary(lmtest)\nm = lm(r ~ 1+zlag(r))\nbgtest(m,order=15)\n#Since p value is greater than α=0.05, we have 95% confident that the residuals of the model are uncorrelated, according to results of Breusch-Godfrey Test.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since p value is greater than α, we are 95% confident that the residuals of the model are uncorrelated, according to results of Box-Ljung Test.\n\nFor the heteroscedasticity, the ACF-PACF plots of the squared residuals should be looked.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check Heteroscedasticity\nrr=r^2\npar(mfrow=c(1,2))\nacf(as.vector(rr),main=\"ACF of Squared Residuals\") \npacf(as.vector(rr),main=\"PACF of Squared Residuals\")\n#Both plots shows that there is no spike out of the white noise bands that is an indication of homoscedasticity\n\n#Breusch-Pagan\nm = lm(r ~ data_train+zlag(data_train)+zlag(data_train,2))\nbptest(m)\n\n#White Test\nm = lm(r ~ data_train+zlag(data_train)+zlag(data_train,2)+zlag(data_train)^2+zlag(data_train,2)^2+zlag(data_train)*zlag(data_train,2))\nbptest(m)\n#All tests show that there is no heteroscedasticity problem.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both plots shows that there is no spike out of the white noise bands that is an indication of homoscedasticity."},{"metadata":{"trusted":true},"cell_type":"code","source":"\narchTest(r)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is homoscedastic, according to archTest. All tests show that there is no heteroscedasticity problem.\nAs a result, all of the assumptions checked.\n\nForecasts are obtained from SARIMA(1,1,1)(2,0,2)[12], Holt’s exponential smoothing method, and neural networks (nnetar). \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#arimafit\nforecast1=forecast(fit4,h=12)\nforecast1\nplot(forecast1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ets\nmodel2<-ets(train_t,model=\"ZZZ\")\nforecast2=forecast(model2,h=12)\nplot(forecast2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nnetar\nmodel3=nnetar(train_t)\nforecast3<-forecast(model3,h=12,PI=T)\nplot(forecast3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since at the beginning the series is transformed for SARIMA model, back transform is done the series to reach the estimates for the original units. \nAt the end,the forecast accuracy measures are calculated for each model"},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast_for_sarima_t=InvBoxCox(as.numeric(forecast1$mean),lambda=lambda)\nforecast_for_ets_t=InvBoxCox(as.numeric(forecast2$mean),lambda)\nforecast_for_nnar_t=InvBoxCox(as.numeric(forecast3$mean),lambda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy(forecast_for_sarima_t,data_test)#Accuracy for SARIMA\naccuracy(forecast_for_ets_t,data_test)#Accuracy for ETS\naccuracy(forecast_for_nnar_t,data_test)#Accuracy for NNAR\n#MApe value of ETS is the lowest,so it is the best technique to forecast","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For SARIMA\nplot(forecast1)\nlines(fit4$fitted,col=\"red\",lty=3)\nlegend(\"bottomright\",c(\"Actual\",\"Fitted\"),col=c(\"black\",\"red\"),lty = c(1,3))\n#For ETS\nplot(forecast2)\nlines(model2$fitted,col=\"red\",lty=3)\nlegend(\"bottomright\",c(\"Actual\",\"Fitted\"),col=c(\"black\",\"red\"),lty = c(1,3))\n#For NNAR\nplot(forecast3)\nlines(model3$fitted,col=\"red\",lty=3)\nlegend(\"bottomright\",c(\"Actual\",\"Fitted\"),col=c(\"black\",\"red\"),lty = c(1,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ETS is the best technique to forecast since its MAPE value is the lowest"},{"metadata":{},"cell_type":"markdown","source":"# CONCLUSION"},{"metadata":{},"cell_type":"markdown","source":"\nIn conclusion, firstly the time series is plotted to understand behaviour. And it is found that series is non-stationary and follows an increasing trend. Then, data is splitted into two parts in order to measure accuracy of forecasts. Box Cox transformation is applied to stabilize variance after that. Later on, anomaly detection is made and if the series is cleaned from anomalies. KPSS test shows that the system is non-stationary and follows stochastic trend. Moreover, ADF and PP tests also show the series is non-stationary(unit root). For unit roots, HEGY test is used and found that there are both regular and seasonal unit roots.By taking both regular and seasonal differences are solved both regular and seasonal unit root problems. The series also became stationary. Then, by looking ACF and PACF plots model is suggested. Also, from auto.arima function a model is suggested. These models are SARIMA(2,1,2)(2,1,2)[12] and SARIMA(1,1,1)(2,0,2)[12].But only SARIMA(1,1,1)(2,0,2)[12] is significant. Moreover, diagnostics are checked for this model and all of the assumptions are checked.\nSecondly, forecasts are obtained with SARIMA model, ETS and neural networks. And back transformation is done because at first Box-Cox transformation is applied. Accuracy measurements are done for forecasts. Moreover, it is found that ETS is the best technique to forecast since its MAPE value is the lowest.\n\n"}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}